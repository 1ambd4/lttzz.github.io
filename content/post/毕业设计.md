---
title: "毕业设计"
date: 2021-01-17T11:49:27+08:00
lastmod: 2021-01-17T11:49:27+08:00
draft: false
keywords: []
description: ""
tags: ["andorid"]
categories: []
author: "lttzz"

# You can also close(false) or open(true) something for this content.
# P.S. comment can only be closed
comment: false
toc: true
autoCollapseToc: true
postMetaInFooter: false
hiddenFromHomePage: false
# You can also define another contentCopyright. e.g. contentCopyright: "This is another copyright."
contentCopyright: false
reward: false
mathjax: true
mathjaxEnableSingleDollar: true
mathjaxEnableAutoNumber: true

# You unlisted posts you might want not want the header or footer to show
hideHeaderAndFooter: false

# You can enable or disable out-of-date content warning for individual post.
# Comment this out to use the global config.
#enableOutdatedInfoWarning: false

flowchartDiagrams:
  enable: false
  options: ""

sequenceDiagrams: 
  enable: false
  options: ""

---

毕设选题：个性化新闻阅读系统移动端的设计与实现

<!--more-->

# 0x00 序言&预备知识

早受够了乱七八糟的APP，想自己写一套资讯聚合系统，利用碎片化的时间了解近期社会、行业的动态，刚巧毕设系统看到了，赶忙选了，ddl在那儿，没借口不去实现了吧 ~~似乎给自己挖了个坑~~。

## UI设计

[1小时掌握Adobe Xd](https://www.bilibili.com/video/BV1zK4y1v7wy)

[超人的电话亭公开课 - XD高级交互动画详解](https://www.bilibili.com/video/BV1gt411h7Aw)

## Android

[Android实战项目: 视频资讯APP](https://www.bilibili.com/video/BV16Z4y1H7jj)

[阿里巴巴Android开发手册](https://yq.aliyun.com/attachment/download/?id=5259)

## Java

[Java零基础教程视频](https://www.bilibili.com/video/BV1Rx411876f)

## Django

[通俗易懂Python Django网站开发绝对零入门(1小时入门)第一季](https://www.bilibili.com/video/BV1Wt411K7QH)

[Python全栈（老男孩4期）Django框架入门到应用](https://www.bilibili.com/video/BV1mW411G7g6)

## LaTeX

[Overleaf入门，带你快速上手LaTeX](https://www.bilibili.com/video/BV1fA411W7kZ)

[Overleaf进阶，更多技能等你解锁](https://www.bilibili.com/video/BV1Ma4y1p7Q6)

[Overleaf高阶，收割更多技能](https://www.bilibili.com/video/BV1aK411u7us)

# Docker

[《第一本Docker书》](https://book.douban.com/subject/26285268/)



# 0x01 爬虫

+ 爬虫：模拟客户端，发送请求，解析响应。
+ lowbi 浏览器我真的笑了
  + 'User-Agent' : 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'

## 黑马爬虫课程

+ 善用无痕模式

### 爬虫基础

#### 爬虫的作用

+ 数据采集
+ 软件测试
+ 抢票刷票
+ 网络安全

#### 爬虫的分类

+ 爬虫规模
  + 通用爬虫
  + 聚焦爬虫
+ 目的
  + 功能性爬虫（不获取数据，只为实现某一个功能，如抢票刷票短信轰炸等）
  + 增量式爬虫（url、数据变化）

#### http协议

爬虫关心的数据部分：

+ 请求头
  + **Host**
  + **User-Agent**
  + Connection
  + Upgrade-Insecure-Requests（升级为https）
  + **Referer（用于反爬和防盗链）**
  + Content-Type
  + **Cookie**
  + Authorization
+ 响应头
  + **Set-Cookie**

状态码：

+ 200 成功
+ 302 跳转
+ 303 POST重定向
+ 307 GET重定向
+ 403 资源不可用
+ 404 找不到页面
+ 500 服务器内部错误
+ 503 服务器由于维护或者负载过重未能应答

所有的状态码都不可信，一切以是否从抓包得到的数据中获取到的数据为准。（network中抓包得到的源码才是判断依据，elements中的数据是渲染后的数据，不能作为判断依据。）

浏览器发送所有的请求并进行渲染，爬虫只发送指定的请求不进行渲染。

#### 抓包

html静态文件（骨骼）、js/ajax请求（肌肉）、css/font/图片（皮肤）

### requests模块

知识点：

+ header参数的使用
+ 发送带参数的请求
+ header中携带cookie
+ cookie参数的使用
+ cookieJar的转换方式
+ timeout的使用
+ 代理ip参数proxies的使用
+ 使用verify参数忽略CA证书
+ 发送post请求
+ 利用requests.session进行状态保持

#### 代理

+ 正向代理：vpn（知道最终服务器的地址）
+ 反向代理：nginx

##### 代理ip的分类

+ 按照匿名度
  + 透明代理（知道
  + 匿名代理
  + 高密代理
+ 按协议分类
  + http
  + https
  + socks

#### 使用verify参数忽略CA证书

#### response

text			 str

content	  byte

#### 发送post请求

登录、注册、发送大文本的时候，需要用到post请求。

``` python
# 金山词霸查询
import requests
import json
import sys

class Iciba(object):

    def __init__(self, word):
        self.url = 'https://ifanyi.iciba.com/index.php?c=trans&m=fy&client=6&auth_user=key_ciba&sign=dcd0cdb9563fcec1'
        self.headers = {
                'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36',
                }
        self.data = {
                'from' : 'en',
                'to' : 'auto',
                'q' : 'china',
                }

    def get_data(self):
        response = requests.post(self.url, headers=self.headers, data=self.data)
        return response.content

    def parse_data(self, data):
        result = json.loads(data)
        print(result['content']['out'])

    def run(self):
        response = self.get_data()
        self.parse_data(response)

if __name__ == '__main__':
    iciba = Iciba(sys.argv[1])
    iciba.run()
```

##### post数据来源

+ 固定值（金山词霸
  + 抓包比较不变值
+ 输入值
  + 抓包比较变化值
+ 预设值（百度翻译token）
  + 静态文件中的预设值：需要提前从静态html中获取
+ 预设值（github登录token）
  + 需要对指定的地址发送请求获取
+ 在客户端生成的（有道翻译sign）

#### requests.session进行状态保持

session类可以自动处理cookie，即下一次的请求自动带上前一次的cookie

``` python
session = requests.session()
response = session.get(url, headers, ...)
response = session.post(url, data, ...)
```

抓包的时候，勾选一下Preserve log。

 ``` python
import requests
import re
import time
import sys

def login():
    # session
    session = requests.session()

    # headers
    session.headers = {
            'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36'
            }

    # url1 - 获取token
        # 发送请求获取响应
        # 正则提取token
    url1 = 'https://github.com/login'
    res1 = session.get(url1).content.decode()
    authenticity_token = re.findall('<form action="/session" accept-charset="UTF-8" method="post"><input type="hidden" name="authenticity_token" value="(.*?)" />', res1)[0]
    timestamp_secret = re.findall('<input type="hidden" name="timestamp_secret" value="(.*?)" class="form-control" />', res1)[0]
    print(authenticity_token)
    print(timestamp_secret)

    # url2 - 登录
        # 构建发送表单
        # 发送登录请求
    url2 = 'https://github.com/session'
    data = {
            'commit' : 'Sign in',
            'authenticity_token' : authenticity_token,
            'login' : sys.argv[1],
            'password' : sys.argv[2],
            'trusted_device' : '',
            'webauthn-support' : 'supported',
            'webauthn-iuvpaa-support' : 'unsupported',
            'return_to' : '',
            'allow_signup' : '',
            'client_id' : '',
            'integration' : '',
            'required_field_4ffc' : '',
            'timestamp' : str(time.time()).replace('.', '')[0:13],
            'timestamp_secret' : timestamp_secret,
            }
    res2 = session.post(url2, data=data)

    # url3 - 验证是否登录成功
    url3 = 'https://github.com/lttzz'
    res3 = session.get(url3)

    with open('github.html', 'wb') as f:
        f.write(res3.content)

if __name__ == '__main__':
    login()
 ```



### 数据提取

#### 响应内容的分类

+ 结构化数据
  + json（高频出现）
    + json
    + re
    + jsonpath
  + xml（低频出现）
    + re
    + lxml：xpath
+ 非结构化数据
  + html：
    + re
    + lxml：xpath
    + beautifulsoup：xpath、正则、css选择器
    + pyquery：css选择器

#### html和xml的区别

html的标签固定，被设计为显示数据以及如何更好的显示数据，侧重点是数据的显示。

xml标签可以自定义，被设计为传输和存储数据，焦点是数据的内容。

#### jsonpath

使用场景：多层嵌套的复杂字典提取数据

##### 语法规则（返回的结果为列表）

| 表达式 |   描述   |
| :----: | :------: |
|   $    |  根节点  |
|   .    |  字节点  |
|   ..   | 子孙节点 |



``` python
data={'key1':{'key2':{'key3':{'key4':{'key5':{'key6':{'key7':'python'}}}}}}}

# str to json
json.loads(string)

# json
data['key1']['key2']['key3']['key4']['key5']['key6']['key7']

# jsonpath
jsonpath.jsonpath(data, '$..key7')[0]
```

#### lxml

``` python
from lxml import etree

# etree.HTML()能够补全缺失的html标签
html = etree.HTML("html_text")
result = html.xpath("xpath_string") 
```



#### xpath基础语法

|  表达式  |         描述         |
| :------: | :------------------: |
| nodename |       选中元素       |
|    /     |  绝对路径（子节点）  |
|    //    | 相对路径（子孙节点） |
|    .     |       当前节点       |
|    ..    |        父节点        |
|    @     |     选取标签属性     |
|  text()  |   选取标签间的文本   |

 #### xpath节点修饰语法

+ 通过索引值修饰节点
  + /html//div[last()-1]
  + /html//div[postion()>10]
+ 通过属性值修饰节点
  + /html//div[@id="main"]
+ 通过子节点值修饰节点
  + //span[i>1000]             span下有i属性表示用户评级，选择评级大于1000的
+ 通过包含修饰（文本包含，属性包含）
  + //span[contains(text(), "下一页")]       文本包含，文本中包含下一页
  + //div[contains(@id, "qiushi_tag_")]      属性包含，属性中的有id值，后面的字符串用于进一步匹配，如：qiushi_tag_124254747

#### xpath选择未知节点的语法（通配符）

| 通配符 |        描述        |
| :----: | :----------------: |
|   *    |    匹配任何节点    |
|   @*   | 匹配任何属性的节点 |
| node() | 匹配任意类型的节点 |

#### xpath其他语法

复合语法：｜

//span[i>1000] | //span[i>500]

选择$i>1000$或者$i<500$，并不会造成短路。

#### 爬百度贴吧

+ User-Agent
+ json.dumps

``` python
import requests
import json
from lxml import etree

class Tieba(object):

    def __init__(self, name):
        self.url = "https://tieba.baidu.com/f?kw={}&ie=utf-8".format(name)
        self.headers = {
                'User-Agent' : 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'
                # 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36'
                }

    def get_data(self, url):
        response = requests.get(url, headers=self.headers, timeout=5)
        return response.content

    def parse_data(self, data):
        html = etree.HTML(data)
        el_list = html.xpath('//*[@id="thread_list"]/li//a[contains(@href, "/p/")]')

        # print(len(el_list))

        data_list = []
        for el in el_list:
            temp = {}
            temp['title'] = el.xpath('./text()')
            temp['url'] = 'https://tieba.baidu.com' + el.xpath('./@href')[0]
            data_list.append(temp)

        try:
            next_url = 'https:' + html.xpath('//a[contains(text(), "下一页")]/@href')[0]
        except:
            next_url = None

        return data_list, next_url

    def save_data(self, data_list):    
        for data in data_list:
            with open('tieba.json', 'a', encoding='utf-8') as f:
                f.write(json.dumps(data, ensure_ascii=False, indent=4))
            print(data)

    def run(self):
        # 1、url
        # 2、headers
        # 3、发送请求获取响应
        next_url = self.url
        while True:
            data = self.get_data(next_url)
            self.parse_data(data)
            # 4、提取数据
            data_list, next_url = self.parse_data(data)
            self.save_data(data_list)
            print(next_url)
            # 5、判断是否结束
            if next_url == None:
                break

if __name__ == '__main__':
    tieba = Tieba('斋藤飞鸟')
    tieba.run()
```

### selenium

PhantomJS是一款基于webkit的无界面浏览器，利用浏览器原生API，封装成一套更加面向对象的Selenium WebDriver API，直接操作浏览器页面里的元素，甚至直接操作浏览器本身。 

chrome也支持了无头模式，创建浏览器对象的时候传入适当的参数即可。代理、user-agent等也是照此配置。

click()函数的调用需要待点击元素在当前浏览器页面上可以看到，否则无法点击，报错unknow error。

##### 定位元素

+ find_element_by_id
+ find_element(s)_by_xpath
+ find_element(s)_by_css_selector
+ find_element(s)_by_name
+ find_element(s)_by_class_name
+ find_element(s)_by_link_text
+ find_element(s)_by_partial_link_text
+ find_element(s)_by_tag_name  （只能返回定位出来的标签中的第一个）



find_element_by_xxx：定位到则是一个对象，定位不到则报错

find_elements_by_xxx：定位到则是一个含有元素的列表，定位不到则是一个空列表。

frame需要使用`driver.switch_to.frame()` 切换。

#### 等待

+ 强制等待：即time.sleep()，不智能，太长太短无法精确
+ 隐式等待：`drive.implicitly_wait()`类似timeout，对后续都生效
+ 显式等待：明确的等待某一个元素，多用在软件测试中。

### 反爬与反反爬

三月爬虫我哭了，明明四月才开始嗷。

#### 反爬的方向

+ 基于身份识别进行反爬
+ 基于爬虫行为进行反爬
+ 基于数据加密进行反爬

#### 基于身份识别的反爬

+ 请求头
  + User-Agent：尽量使用User-Agent池
  + Referer：正常浏览很多都会带这个参数，但爬虫不会，添加之
  + cookie
+ 请求参数
  + 参数在html静态文件中（github登录）
  + 发送请求获取数据
  + 通过js生成请求参数
  + 通过验证码反爬

#### 基于爬虫行为的反爬

+ 请求频率和请求总数：ip/账号和请求频率、请求间隔、请求数量等的关系
+ 通过js实现的跳转进行反爬：无法在源码中找到下一页的url
+ 通过蜜罐获取爬虫ip和代理ip进行反爬：某些正常用户不可能访问到的数据设置蜜罐，疯掉ip
+ 通过加数据进行反爬：增大数据清洗难度
+ 阻塞任务队列
+ 阻塞网络IO：在蜜罐中混入大文件的url，占用网络IO性能
+ 运维平台综合审计

#### 基于数据加密的反爬

对响应的数据进行处理

+ css数据偏移（去哪儿的票价）
+ 自定义字体
+ 数据加密
+ 数据图片化（58同城短租）
+ 特殊格式编码
+ js动态生成数据

### 验证码

处理方案：

+ 手动输入
+ ocr：tesseract、腾讯/阿里ocr平台
+ 打码平台

对于点击后会变化的验证码，常用selenium截屏后将验证码剪裁出来后在处理验证码。

#### tesseract

GitHub : [tesseract-ocr/tesseract](https://github.com/tesseract-ocr/tesseract)

``` bash
$ pip install pillow
$ pip install pytesseract

# tesseract的简单使用
from PIL import Image
import pytesseract

im = Imgae.open('image_path')

result = pytesseract.image_to_string(im)

print(result)
```

#### js解析

+ 1、定位js文件
  + 通过initiator定位到js文件
  + 通过搜索关键字定位到js文件
  + 分析元素（如按钮）上的事件监听器
+ 2、分析js代码（可以添加断点调试）
+ 3、模拟加密步骤
  + 调用第三方类库，js2py、pyv8、execjs
  + 用python重写代码

#### js2py（人人网登录）

``` python
import js2py

# 创建环境
context = js2py.EvalJs()

# 加载js
xxxJS = request.get().content.decode()
context.execute(xxxJS)


```

#### hashlib

#### 去重

地址去重：

+ url
+ hash(url)
+ 布隆过滤器

文本内容去重：

+ 编辑距离
+ simhash（模糊哈希）

### MongoDB数据库

#### MongoDB的介绍与安装

端口号：27017

默认配置文件：/etc/mongod.conf

配置文件：

``` bash
# 使用-f指定
dbpath=~/mongodb
logpath=~/mongodb/mongodb.log
logappend=true
fork=true
auth=true
bind_ip=0.0.0.0
```

启动方式：

+ 测试方式启动
+ 生产环境启动

SQL：数据库 > 表 > 数据

NoSQL：数据库 > 集合 > 文档

##### 测试方式启动

``` bash
# Ubuntu18.04
# https://mirror.tuna.tsinghua.edu.cn/help/mongodb/

#首先信任 MongoDB 的 GPG 公钥:
wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -

# 再选择你的 Debian / Ubuntu 版本，文本框中内容写进 /etc/apt/sources.list.d/mongodb.list
# 你的 Debian/Ubuntu 版本: 
#   Ubuntu 18.04 LTS
deb https://mirrors.tuna.tsinghua.edu.cn/mongodb/apt/ubuntu bionic/mongodb-org/4.4 multiverse

# 安装 mongodb-org 即可
sudo apt-get update
sudo apt-get install -y mongodb-org

# 启动
service mongod start
```

##### 生产环境启动

``` bash
mongod [option]
```



| 参数               | 作用                      |
| :----------------- | ------------------------- |
| --dbpath           | 指定数据库存放路径        |
| --logpath          | 指定日志的存放路径        |
| --append/logappend | 以追加模式写入日志        |
| --fork             | 开启新进程运行mongodb服务 |
| -f PATH            | 加载配置文件              |
| --auth             | 以权限认证的方式启动      |

#### MongoDB命令

``` bash
# 数据库命令
db													显示当前的数据库
show databases/show dbs			查看所有的数据库
use db_name									使用数据库
db.dropDatabase()						删除当前的数据库

# 集合命令
# 无需手动创建，向不存在的集合第一次插入数据的时候，集合会被创建
# 手动创建
db.createCollection(name, option)
db.createCollection("stu", {capped:true, size:10})  # 当文件大小超过预设值时，会覆盖数据
show collections

# 增删改查
# COLLECTION 表示集合名称
db.COLLECTION.insert({})						   # 单一插入
db.COLLECTION.insert([{}, {}, ...])    # 批量插入，效率高
# upsert （update or insert）
db.COLLECTION.save({_id:'V', xxx:yyy, ...})
db.COLLECTION.save({xxx:yyy, ...})
# 查询操作
db.COLLECTION.find()
db.COLLECTION.find().pretty()
		$lt
		$lte
		$gt
		$gte
		$ne
db.COLLECTION.find({age:18})				 # 年龄为18
db.COLLECTION.find({age:{$gt:18}})   # 年龄大雨18
# 复合查询（逻辑运算符）
db.COLLECTION.find({$and[{age:18}, {gender:false}]})
db.COLLECTION.find({$or[{age:18}, {gender:false}]})
# 范围运算符
db.COLLECTION.find({age:{$in:[18, 19, 20]}})
# 正则查询
db.COLLECTION.find({name:[$regex:"^李"]})
# 自定义函数
db.COLLECTION.find({$where:function()
		 {return this.age>30}
})
# skip 和 limit
db.COLLECTION.find().skip(0).limit(2)
db.COLLECTION.find().skip(2).limit(2)
# 投影操作，类比mysql中 select name,age from stu;
db.COLLECTION.find({}, {name:1, age:1})		# 想看的字段用1
db.COLLECTION.find({}, {_id:0, name:1, age:1}) # 只有_id可以0和1共存，否则要不全是0，要不全是1
# 排序，1为升序，2为降序
db.COLLECTION.find().sort({age:1, gender:-1})
# 去重
db.COLLECTION.distinct("gender", {age:18})   # 在年龄为18的查询结果中对性别去重
# 更新，query：查询条件；update：更新操作；multi：默认只更新第一条
db.COLLECTION.update({query}, {update}, {multi:boolean})
db.COLLECTION.update({name:"li"}, {$set:{age:18}}, {upsert:true})			# 以upsert模式将姓名为li的年龄更新/设置为18
# 删除
db.COLLECTION.remove({})

# 管道命令
	$group		分组
	$match		过滤数据
	$project	投影操作， 修改输入文档的结构，如重命名、增/删字段、创建计算结果
	$sort			将输入文档排序后输出
	$limit		限制返回文档的数量
	$skip			跳过指定数量的文档，返回余下的
```

| 类型      | 说明                            |
| --------- | ------------------------------- |
| Object ID | 文档ID/数据ID                   |
| String    | 字符串，必须是有效的UTF-8字符串 |
| Boolean   |                                 |
| Integer   |                                 |
| Double    |                                 |
| Arrays    |                                 |
| Object    |                                 |
| Null      |                                 |
| Timestamp |                                 |
| Date      |                                 |

#### 索引

加快查询速度。

#### 权限管理

``` bash
# 使用admin数据库，超级管理员必须创建在该数据库上
use admin
# 创建超级用户
db.createUser({user:"username", pwd:"password", roles:["root"]})
# roles还可以是：read、readWrite
# 登录（在哪里创建在那里登录）
db.auth("username", "password")

# 显示用户
show users

# 配置对于多个数据库具有操作权限，需要在admin上创建
db.createUser({user:"username", pwd:"password", roles:[{db:"db1", roles:"read"}, {db:"db2", roles:readWrite}]})

# 删除用户，需要在超级管理员用户下操作
db.dropUser("username")
```

#### pymongo

``` python
from pymongo import MongoClient

# 创建数据库链接对象
client = MongoClient('xx.xx.xx.xx', 27017)

# 选择一个数据库
db = client['admin']
# 认证
db.authenticate('username', 'password')

# 选择一个集合
col = client['itheima']['test']


# 增删改查
# col.insert({"python":"python3.7"})
# col.insert({"C++":"C++17"})

for data in col.find():
    print(data)

col.update({"python":"python3.7"}, {"python":"python3.9"}, {multi:boolean})

for data in col.find():
    print(data)
```



### scrapy基础

#### 各个模块的功能

+ 引擎：数据和信号的传递
+ 调度器：任务队列
+ 下载器
+ 爬虫：起始url、解析
+ 管道：保存数据
+ 中间件：定制化操作

#### scrapy项目开发流程

``` python
# 1、创建项目
scrapy startproject news
# x 、建模
# 2、生成爬虫
scrapy genspider example example.com
# 3、提取数据
根据网站结构在spider中实现数据采集相关的内容
# 4、保存数据
使用pipline进行数据后续处理和保存
# 5、运行爬虫
scrapy clawl example --nolog
```

 #### scrapy编写爬虫

``` bash
# 1、修改起始url
# 2、修改允许的域
# 3、编写解析函数
```

#### scrape 中间件

+ 下载中间件
+ 爬虫中间件

中间件的使用方法：

``` python
# 1、在middleware.py中定义中间件类

# 2、在中间件类中重写处理请求或响应的方法
		process_request(self, request, spider)
  			当每个request经过中间件的时候被调用
    		None			如果所有中间件都返回None则交给下载器
      	request	  如果返回为请求，则交给调度器
        response	将响应对象交给spider进行解析
  
  	process_response(self, requset, response, spider)
    		当下载器完成http请求，传递响应给引擎的时候调用
      	request	  如果返回为请求，则交给调度器
        response	将响应对象交给spider进行解析    
        
# 3、在settings文件中开启中间件，权重越小优先级越高
```



### scrapy-redis

通过持久化请求队列和请求的指纹集合来实现：

+ 断点续爬
+ 分布式快速爬取

#### scrapy_radis配置

``` python
# 指纹生成以及去重类
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" 

# 调度器类
SCHEDULER = "scrapy_redis.scheduler.Scheduler" 

# 持久化请求队列和指纹集合
SCHEDULER_PERSIST = True 

# 数据存入redis的管道
ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400} 

# redis的url
REDIS_URL = "redis://host:port" 
```



#### 分布式爬虫编写流程

``` python
# 1、编写普通爬虫
		创建项目
  	明确目标
    创建爬虫
    保存内容
#2、改造成分布式爬虫
		1.改造爬虫
  			导入scrapy_redis中的分布式爬虫类
    		继承类
      	注销 start_urls 和 allow_domains
        设置redis_key获取start_urls
        设置 __init__ 获取允许的域名
     2.改造配置文件
        copy配置参数
```



### scrapy其他

#### scrapy_splash

模拟浏览器加载js，并且返回js运行后的数据。

##### 安装

``` bash
# 1、splash
docker
# 2、python模块
pip install scrapy-splash
```

#### 使用

1. 运行splash服务

2. 在代码中使用

   1. 创建项目

   2. 创建爬虫

   3. 修改配置文件

   4. 编写代码（重写start_requests方法，创建SplashRequest请求）



#### scrapy的日志信息和配置

##### scrapy常用配置

+ ROBOTSTXT_OBEY：是否遵循爬虫协议

+ UEER_AGENT：ua
+ DEFAULT_REQUEST_HEADERS：默认的请求头
+ COOKIES_ENABLED：是否开启cookies
+ COOKIES_DEBUG：日志中是否显示cookies相关信息
+ LOG_LEVEL：日志等级
+ LOG_FILE：
+ DOWNLOAD_DELAY：
+ CONCURRENT_REQUESTS：并发量
+ CONCURRENT_REQUESTS_PER_DOMAIN
+ CONCURRENT_REQUESTS_PER_IP：



#### scrapyd

``` bash
# server
pip install scrapyd
# client
pip install scrapy-client

# 配置 scrapy.cfg
[deploy:部署名]
url = http://localhost:6800/
project = JD		# 这儿的是项目名

# 部署
scrapyd-deploy 部署名 -q 项目名

# 启动爬虫
curl http://localhost:6800/schedule.json -d project=project_name -d spider=spider_name
```

#### gerapy

基于scrapyd的分布式爬虫管理框架



### appium



## 增量式爬虫

新闻网站会不定期的更新，不论是新页面的产生，还是原页面的内容被修改，这些变化都可视作增量，需要让爬虫检测网站更新情况以获得最新的数据。爬虫爬取的过车过是“请求 -> 响应 -> 解析 -> 存储”，那么思路有：

+ 发送请求前判断URL是否爬取过（适用于网站更新但原内容不更新的情况）
+ 解析内容后判断内容是否之前爬取过（适用于页面内容更新的情况）
+ 持久化存储的时候判断是否已在存储介质中

增量式爬虫的核心在于去重，也即使用数据指纹，常用的是哈希值。除了数据指纹，参考的文章中还提到了304状态码、Last-modified字段、文件大小、MD5签名等。

### BloomFilter

常年不更新了，不支持python3。

### Redis



### 定时爬虫

``` python
# 每天的十点启动
from apscheduler.schedulers.blocking import BlockingScheduler

def run():
    pass


sched = BlockingScheduler()
sched.add_job(run, 'cron', hour=10, minute=0)
sched.start()


# 每隔三分钟启动一次
from apscheduler.schedulers.blocking import BlockingScheduler

def run():
    pass


sched = BlockingScheduler()
sched.add_job(run, 'interval', minutes=3)
sched.start()
```





## Scrapy

文档：[scrapy documentation](https://docs.scrapy.org/en/latest/)



## 参考

[爬虫的增量式抓取和数据更新](https://www.jianshu.com/p/7f2be7eed247)

